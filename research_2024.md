## Собранный плейлист с референсами:

[Плейлист](https://www.youtube.com/playlist?list=PLgjfdixKuQ0o6VFBccIoWYfCdlonrlhK7&jct=HyhHmAy1yisdbpkdwQbsJQ)

Основное:

-   [Turn yourself into ANY STYLE with ComfyUI [PS1, 3D Animation, Anime…]](https://www.youtube.com/watch?v=efxYEbF793c&list=PLgjfdixKuQ0o6VFBccIoWYfCdlonrlhK7&index=9&ab_channel=Mickmumpitz)
-   []()

## Kaggle модели

https://www.kaggle.com/models?task=16702&orderby=voteCount

Библиотека предварительно обученных моделей, загруженных сообществом Kaggle. Она предназначена для ускорения работы разработчиков и исследователей машинного обучения. Основные особенности раздела:

## Разное

---

https://github.com/ZhouYiiFeng/TDMS-Net?ysclid=m41eviibux440370672

Temporal Denoising Mask Synthesis Network (TDMS-Net), предназначена для улучшения временной согласованности видео. Модель TDMS-Net решает проблему мерцания и несогласованности, возникающую при обработке видео кадр за кадром стандартными алгоритмами обработки изображений.

---

https://github.com/ChenyangLEI/deep-video-prior?tab=readme-ov-file
Deep Video Prior (DVP) реализация метода временной согласованности видео с использованием глубинного обучения. Предлагает подход к улучшению согласованности видео, обрабатываемого алгоритмами обработки изображений.

---

Повышение временной согласованности видео, обработанного такими методами, как окрашивание или удаление дымки, что позволяет преобразовать алгоритмы для изображений в алгоритмы для видео.

https://github.com/marceljmueller/vid2vid?ysclid=m41expe0bv221299258

---

Проект для видео-видео синтеза на основе модели pix2pix GAN. Основная идея — преобразование видео с сохранением контента одного персонажа, но с использованием стиля другого. Реализация включает этапы предварительной обработки данных, обучения модели и постобработки для создания видео. Для работы требуются Python, GPU и CUDA. Процесс подробно описан, включая использование заранее подготовленных данных и возможность настройки параметров. Результаты сохраняются с объединением аудио и видео.

---

https://github.com/Libyte/mmediting/blob/main/configs/flavr/README.md?ysclid=m41f39v2es682915267

предоставляет конфигурации для модели FLAVR, предназначенной для интерполяции видео (добавления промежуточных кадров). Основное применение — плавное преобразование кадров для повышения качества видео. Репозиторий содержит описание структуры конфигурационных файлов, включая данные для обучения и настройки модели. Используется фреймворк PyTorch и инструменты из экосистемы MMEditing.

---

https://github.com/nerdyrodent/VQGAN-CLIP

предоставляет инструменты для локального запуска VQGAN+CLIP, модели генерации изображений из текстовых описаний. Проект основан на PyTorch, поддерживает пользовательские текстовые и стилевые подсказки, создание видео и трансформацию изображений. Включены инструкции по установке, настройке окружения, загрузке моделей и запуску генерации. Примеры показывают применение "стилевого переноса" и сюжетных генераций. Требуется GPU для оптимальной производительности.

---

## Для работы с семейством моделей Stable

https://github.com/Stability-AI/generative-models

https://github.com/nateraw/stable-diffusion-videos

Модульные генеративные модели, включая text-to-video, video-to-video и text-to-video. Поддерживает настройки, тренировку, inference, а также кастомизацию моделей через YAML. Используются PyTorch Lightning и инструменты для обработки данных. Доступны примеры моделей, такие как SDXL 1.0 и Stable Video Diffusion.

---

## Материалы от Дмитрия Валерьевича Сошникова:

https://github.com/shwars/AI_Art_Workbooks/blob/main/StyleTransfer.ipynb

https://github.com/shwars/AI_Art_Workbooks/blob/main/StableDiffusionWorkbook.ipynb

https://github.com/shwars/AI_Art_Workbooks/blob/main/StableDiffusionLatentVideo.ipynb

перенос стиля с использованием техники Neural Style Transfer (NST).
